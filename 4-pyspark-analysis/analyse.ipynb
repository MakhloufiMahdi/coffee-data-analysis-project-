{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adcdeb3b-0c34-4e7f-b902-948963003dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+----------------+---------------+--------+---------------+----------+----------+------------------+--------------------+--------------------+\n",
      "|transaction_id|transaction_date|transaction_time|transaction_qty|store_id| store_location|product_id|unit_price|  product_category|        product_type|      product_detail|\n",
      "+--------------+----------------+----------------+---------------+--------+---------------+----------+----------+------------------+--------------------+--------------------+\n",
      "|             1|      2023-01-01|        07:06:11|              2|       5|Lower Manhattan|        32|       3.0|            Coffee|Gourmet brewed co...|         Ethiopia Rg|\n",
      "|             2|      2023-01-01|        07:08:56|              2|       5|Lower Manhattan|        57|       3.1|               Tea|     Brewed Chai tea|Spicy Eye Opener ...|\n",
      "|             3|      2023-01-01|        07:14:04|              2|       5|Lower Manhattan|        59|       4.5|Drinking Chocolate|       Hot chocolate|   Dark chocolate Lg|\n",
      "|             4|      2023-01-01|        07:20:24|              1|       5|Lower Manhattan|        22|       2.0|            Coffee|         Drip coffee|Our Old Time Dine...|\n",
      "|             5|      2023-01-01|        07:22:41|              2|       5|Lower Manhattan|        57|       3.1|               Tea|     Brewed Chai tea|Spicy Eye Opener ...|\n",
      "|             6|      2023-01-01|        07:22:41|              1|       5|Lower Manhattan|        77|       3.0|            Bakery|               Scone|       Oatmeal Scone|\n",
      "|             7|      2023-01-01|        07:25:49|              1|       5|Lower Manhattan|        22|       2.0|            Coffee|         Drip coffee|Our Old Time Dine...|\n",
      "|             8|      2023-01-01|        07:33:34|              2|       5|Lower Manhattan|        28|       2.0|            Coffee|Gourmet brewed co...|Columbian Medium ...|\n",
      "|             9|      2023-01-01|        07:39:13|              1|       5|Lower Manhattan|        39|      4.25|            Coffee|    Barista Espresso|            Latte Rg|\n",
      "|            10|      2023-01-01|        07:39:34|              2|       5|Lower Manhattan|        58|       3.5|Drinking Chocolate|       Hot chocolate|   Dark chocolate Rg|\n",
      "|            11|      2023-01-01|        07:43:05|              1|       5|Lower Manhattan|        56|      2.55|               Tea|     Brewed Chai tea|Spicy Eye Opener ...|\n",
      "|            12|      2023-01-01|        07:44:35|              2|       5|Lower Manhattan|        33|       3.5|            Coffee|Gourmet brewed co...|         Ethiopia Lg|\n",
      "|            13|      2023-01-01|        07:45:51|              1|       5|Lower Manhattan|        51|       3.0|               Tea|    Brewed Black tea|        Earl Grey Lg|\n",
      "|            14|      2023-01-01|        07:48:19|              1|       5|Lower Manhattan|        57|       3.1|               Tea|     Brewed Chai tea|Spicy Eye Opener ...|\n",
      "|            15|      2023-01-01|        07:52:36|              2|       5|Lower Manhattan|        87|       3.0|            Coffee|    Barista Espresso|Ouro Brasileiro shot|\n",
      "|            16|      2023-01-01|        07:59:58|              2|       5|Lower Manhattan|        47|       3.0|               Tea|    Brewed Green tea|Serenity Green Te...|\n",
      "|            17|      2023-01-01|        07:59:58|              1|       5|Lower Manhattan|        79|      3.75|            Bakery|               Scone|  Jumbo Savory Scone|\n",
      "|            18|      2023-01-01|        08:00:18|              1|       8| Hell's Kitchen|        42|       2.5|               Tea|   Brewed herbal tea|      Lemon Grass Rg|\n",
      "|            19|      2023-01-01|        08:00:39|              2|       8| Hell's Kitchen|        59|       4.5|Drinking Chocolate|       Hot chocolate|   Dark chocolate Lg|\n",
      "|            20|      2023-01-01|        08:11:45|              1|       8| Hell's Kitchen|        61|      4.75|Drinking Chocolate|       Hot chocolate|Sustainably Grown...|\n",
      "+--------------+----------------+----------------+---------------+--------+---------------+----------+----------+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-21\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\Users\\\\makhl\\\\spark\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "\n",
    "sys.path.append(os.path.join(os.environ[\"SPARK_HOME\"], \"python\"))\n",
    "sys.path.append(os.path.join(os.environ[\"SPARK_HOME\"], \"python\", \"lib\", \"py4j-src.zip\"))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lire CSV\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"C:\\\\Users\\\\makhl\\\\Coffee Shop Sales.csv\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73a30126-d8a4-46e7-95b9-a9a03d1bb58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|week|weekly_sales|\n",
      "+----+------------+\n",
      "|   1|  17,139.330|\n",
      "|   2|  19,129.530|\n",
      "|   3|  19,818.510|\n",
      "|   4|  18,271.630|\n",
      "|   5|  17,231.590|\n",
      "|   6|  18,333.350|\n",
      "|   7|  19,856.620|\n",
      "|   8|  20,063.070|\n",
      "|   9|  20,389.750|\n",
      "|  10|  22,146.340|\n",
      "|  11|  23,594.590|\n",
      "|  12|  23,395.300|\n",
      "|  13|  22,055.550|\n",
      "|  14|  26,079.890|\n",
      "|  15|  28,981.300|\n",
      "|  16|  29,217.510|\n",
      "|  17|  27,386.630|\n",
      "|  18|  32,110.100|\n",
      "|  19|  36,056.880|\n",
      "|  20|  38,476.550|\n",
      "+----+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, weekofyear, sum as _sum, format_number\n",
    "\n",
    "df = df.withColumn(\"transaction_qty\", col(\"transaction_qty\").cast(\"int\")) \\\n",
    "       .withColumn(\"unit_price\", col(\"unit_price\").cast(\"double\")) \\\n",
    "       .withColumn(\"transaction_date\", col(\"transaction_date\").cast(\"date\"))\n",
    "\n",
    "df = df.withColumn(\"week\", weekofyear(col(\"transaction_date\"))) \\\n",
    "       .withColumn(\"revenue\", col(\"transaction_qty\") * col(\"unit_price\"))\n",
    "\n",
    "weekly_sales = df.groupBy(\"week\") \\\n",
    "    .agg(_sum(\"revenue\").alias(\"weekly_sales\")) \\\n",
    "    .orderBy(\"week\")\n",
    "weekly_sales.select(\n",
    "    \"week\",\n",
    "    format_number(\"weekly_sales\", 3).alias(\"weekly_sales\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0855c248-3d8c-4b3c-84ef-3206a7251b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+\n",
      "|product_category  |average_price|\n",
      "+------------------+-------------+\n",
      "|Bakery            |3.552        |\n",
      "|Loose Tea         |9.267        |\n",
      "|Drinking Chocolate|4.149        |\n",
      "|Tea               |2.817        |\n",
      "|Branded           |17.720       |\n",
      "|Coffee            |3.024        |\n",
      "|Coffee beans      |21.018       |\n",
      "|Packaged Chocolate|9.051        |\n",
      "|Flavours          |0.800        |\n",
      "+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, format_number\n",
    "\n",
    "df_avg_price = df.groupBy(\"product_category\") \\\n",
    "                 .agg(avg(\"unit_price\").alias(\"average_price\"))\n",
    "\n",
    "df_avg_price.select(\"product_category\", format_number(\"average_price\", 3).alias(\"average_price\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c00971ce-0959-49c1-be1f-c67513965dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|avg_basket_value|\n",
      "+----------------+\n",
      "|           4.686|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, format_number\n",
    "\n",
    "df_basket = df.withColumn(\"basket_value\", col(\"transaction_qty\") * col(\"unit_price\")) \\\n",
    "              .agg(avg(\"basket_value\").alias(\"avg_basket_value\"))\n",
    "\n",
    "df_basket.select(format_number(\"avg_basket_value\", 3).alias(\"avg_basket_value\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9286333a-ba67-4370-ae34-359c5332879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----------+\n",
      "|store_id|store_location |total_sales|\n",
      "+--------+---------------+-----------+\n",
      "|8       |Hell's Kitchen |236,511.170|\n",
      "|3       |Astoria        |232,243.910|\n",
      "|5       |Lower Manhattan|230,057.250|\n",
      "+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "df.withColumn(\"revenue\", col(\"transaction_qty\") * col(\"unit_price\")) \\\n",
    "  .groupBy(\"store_id\", \"store_location\") \\\n",
    "  .agg(_sum(\"revenue\").alias(\"total_sales\")) \\\n",
    "  .orderBy(col(\"total_sales\").desc()) \\\n",
    "  .limit(5) \\\n",
    " .select(\"store_id\", \"store_location\", format_number(\"total_sales\", 3).alias(\"total_sales\")) \\\n",
    "  .show(truncate=False)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfd8288d-f46e-4b90-b91c-7690bf55b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+-------------+\n",
      "|category          |nb_sales|total_revenue|\n",
      "+------------------+--------+-------------+\n",
      "|Coffee            |58416   |269952.45    |\n",
      "|Tea               |45449   |196405.95    |\n",
      "|Bakery            |22796   |82315.64     |\n",
      "|Drinking Chocolate|11468   |72416.0      |\n",
      "|Coffee beans      |1753    |40085.25     |\n",
      "|Branded           |747     |13607.0      |\n",
      "|Loose Tea         |1210    |11213.6      |\n",
      "|Flavours          |6790    |8408.8       |\n",
      "|Packaged Chocolate|487     |4407.64      |\n",
      "+------------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT product_category AS category, \n",
    "           COUNT(*) AS nb_sales, \n",
    "           ROUND(SUM(revenue), 3) AS total_revenue\n",
    "    FROM sales\n",
    "    GROUP BY product_category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d18d77dc-3bed-4765-a3bd-3f87cf74fb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+\n",
      "|features |label|prediction|\n",
      "+---------+-----+----------+\n",
      "|[2.0,3.0]|6.000|6.196     |\n",
      "|[2.0,3.1]|6.200|6.314     |\n",
      "|[2.0,4.5]|9.000|7.963     |\n",
      "|[1.0,2.0]|2.000|1.529     |\n",
      "|[2.0,3.1]|6.200|6.314     |\n",
      "+---------+-----+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"transaction_qty\", \"unit_price\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(df.withColumn(\"label\", col(\"revenue\")))\n",
    "\n",
    "# Régression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(data)\n",
    "\n",
    "predictions = model.transform(data)\n",
    "\n",
    "predictions.select(\n",
    "    \"features\",\n",
    "    format_number(\"label\", 3).alias(\"label\"),\n",
    "    format_number(\"prediction\", 3).alias(\"prediction\")\n",
    ").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0ce2c02-680f-4198-9d05-a7dece704c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|day_of_week|total_revenue|\n",
      "+-----------+-------------+\n",
      "|          1|   98,330.310|\n",
      "|          2|  101,677.280|\n",
      "|          3|   99,455.940|\n",
      "|          4|  100,313.540|\n",
      "|          5|  100,767.780|\n",
      "|          6|  101,373.000|\n",
      "|          7|   96,894.480|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofweek, col, sum as _sum, format_number\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df_casted = df.withColumn(\"transaction_qty\", col(\"transaction_qty\").cast(DoubleType())) \\\n",
    "              .withColumn(\"unit_price\", col(\"unit_price\").cast(DoubleType()))\n",
    "\n",
    "df_day = df_casted.withColumn(\"day_of_week\", dayofweek(\"transaction_date\"))\n",
    "\n",
    "df_day.groupBy(\"day_of_week\") \\\n",
    "      .agg(format_number(_sum(col(\"transaction_qty\") * col(\"unit_price\")), 3).alias(\"total_revenue\")) \\\n",
    "      .orderBy(\"day_of_week\") \\\n",
    "      .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a217826d-2218-455f-bcb4-2b7d94b15258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------+\n",
      "|week|weekly_revenue|prev_week|\n",
      "+----+--------------+---------+\n",
      "|   1|      17139.33|      0.0|\n",
      "|   2|      19129.53| 17139.33|\n",
      "|   3|      19818.51| 19129.53|\n",
      "|   4|      18271.63| 19818.51|\n",
      "|   5|      17231.59| 18271.63|\n",
      "|   6|      18333.35| 17231.59|\n",
      "|   7|      19856.62| 18333.35|\n",
      "|   8|      20063.07| 19856.62|\n",
      "|   9|      20389.75| 20063.07|\n",
      "|  10|      22146.34| 20389.75|\n",
      "|  11|      23594.59| 22146.34|\n",
      "|  12|       23395.3| 23594.59|\n",
      "|  13|      22055.55|  23395.3|\n",
      "|  14|      26079.89| 22055.55|\n",
      "|  15|       28981.3| 26079.89|\n",
      "|  16|      29217.51|  28981.3|\n",
      "|  17|      27386.63| 29217.51|\n",
      "|  18|       32110.1| 27386.63|\n",
      "|  19|      36056.88|  32110.1|\n",
      "|  20|      38476.55| 36056.88|\n",
      "+----+--------------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, weekofyear, col, round, sum as _sum\n",
    "df = df.withColumn(\"week\", weekofyear(col(\"transaction_date\")))\n",
    "weekly_revenue = df.groupBy(\"week\").agg(_sum(\"revenue\").alias(\"weekly_revenue\"))\n",
    "w = Window.orderBy(\"week\")\n",
    "weekly_revenue = weekly_revenue.withColumn(\"prev_week\", lag(\"weekly_revenue\", 1, 0).over(w))\n",
    "\n",
    "weekly_revenue = weekly_revenue.withColumn(\"weekly_revenue\", round(col(\"weekly_revenue\"), 3)) \\\n",
    "                               .withColumn(\"prev_week\", round(col(\"prev_week\"), 3))\n",
    "weekly_revenue.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55751f35-0133-4a27-87ec-4a2faf69c6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------------+------------+-------+\n",
      "|product_id|week|weekly_revenue|prev_revenue| growth|\n",
      "+----------+----+--------------+------------+-------+\n",
      "|         1|   5|          54.0|        36.0|   50.0|\n",
      "|         1|   6|         180.0|        54.0|233.333|\n",
      "|         1|  10|         234.0|        18.0| 1200.0|\n",
      "|         1|  14|         180.0|        36.0|  400.0|\n",
      "|         1|  19|         396.0|        18.0| 2100.0|\n",
      "|         1|  23|         360.0|        90.0|  300.0|\n",
      "|        10|   2|          40.0|        20.0|  100.0|\n",
      "|        10|   3|         110.0|        40.0|  175.0|\n",
      "|        10|   6|          30.0|        20.0|   50.0|\n",
      "|        10|   7|         100.0|        30.0|233.333|\n",
      "|        10|  10|          60.0|        40.0|   50.0|\n",
      "|        10|  11|          80.0|        60.0| 33.333|\n",
      "|        10|  16|          90.0|        40.0|  125.0|\n",
      "|        10|  19|          50.0|        30.0| 66.667|\n",
      "|        10|  20|         150.0|        50.0|  200.0|\n",
      "|        10|  23|          70.0|        30.0|133.333|\n",
      "|        10|  24|         100.0|        70.0| 42.857|\n",
      "|        11|   2|         26.85|        8.95|  200.0|\n",
      "|        11|   3|          71.6|       26.85|166.667|\n",
      "|        11|   5|          17.9|        8.95|  100.0|\n",
      "+----------+----+--------------+------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import weekofyear, col, lag, round, sum as _sum\n",
    "\n",
    "df = df.withColumn(\"week\", weekofyear(\"transaction_date\"))\n",
    "\n",
    "product_week = df.groupBy(\"product_id\", \"week\").agg(_sum(\"revenue\").alias(\"weekly_revenue\"))\n",
    "\n",
    "w = Window.partitionBy(\"product_id\").orderBy(\"week\")\n",
    "\n",
    "product_week = product_week.withColumn(\"prev_revenue\", lag(\"weekly_revenue\", 1).over(w))\n",
    "\n",
    "product_week = product_week.withColumn(\n",
    "    \"weekly_revenue\", round(col(\"weekly_revenue\"), 3)\n",
    ").withColumn(\n",
    "    \"prev_revenue\", round(col(\"prev_revenue\"), 3)\n",
    ").withColumn(\n",
    "    \"growth\", round(((col(\"weekly_revenue\") - col(\"prev_revenue\")) / col(\"prev_revenue\")) * 100, 3)\n",
    ")\n",
    "product_week.filter(col(\"growth\") > 30).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1b887f2-a748-40bf-ac08-2e5f56c60345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+---------+----------+\n",
      "|store_id|recency|frequency| monetary|prediction|\n",
      "+--------+-------+---------+---------+----------+\n",
      "|       3|    736|    50599|232243.91|         1|\n",
      "|       8|    736|    50735|236511.17|         2|\n",
      "|       5|    736|    47782|230057.25|         0|\n",
      "+--------+-------+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, datediff, current_date, countDistinct, max, sum as _sum, round\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "df = df.withColumn(\"revenue\", col(\"transaction_qty\") * col(\"unit_price\"))\n",
    "\n",
    "rfm = df.groupBy(\"store_id\").agg(\n",
    "    datediff(current_date(), max(\"transaction_date\")).alias(\"recency\"),\n",
    "    countDistinct(\"transaction_id\").alias(\"frequency\"),\n",
    "    round(_sum(\"revenue\"), 3).alias(\"monetary\")  \n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"recency\", \"frequency\", \"monetary\"], outputCol=\"features\")\n",
    "rfm_vector = assembler.transform(rfm)\n",
    "\n",
    "kmeans = KMeans(k=4, seed=1)\n",
    "model = kmeans.fit(rfm_vector)\n",
    "\n",
    "model.transform(rfm_vector).select(\n",
    "    \"store_id\", \"recency\", \"frequency\", \"monetary\", \"prediction\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09932bf0-44d5-42b2-b787-2592ef2d31cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de corrélation :\n",
      "DenseMatrix([[ 1.        , -0.12354566,  0.35623085],\n",
      "             [-0.12354566,  1.        ,  0.68554956],\n",
      "             [ 0.35623085,  0.68554956,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"transaction_qty\", \"unit_price\", \"revenue\"], outputCol=\"features\")\n",
    "vector_df = assembler.transform(df)\n",
    "\n",
    "corr = Correlation.corr(vector_df, \"features\").head()\n",
    "print(\"Matrice de corrélation :\")\n",
    "print(corr[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737bba7-4ad6-4d6c-b684-4499cd350c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
